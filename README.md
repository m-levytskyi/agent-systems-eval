# Agent Systems Evaluation: Monolithic vs Ensemble

An empirical comparison of a Monolithic Agent (single LLM) vs. a Multi-Agent Ensemble for document synthesis tasks. This project evaluates both approaches using MLflow for experiment tracking and LLM-as-a-judge for quality assessment.

## Overview

This project implements and compares two approaches to document synthesis:

1. **Monolithic Agent** (`monolithic.py`): A single LLM that directly synthesizes source documents according to task requirements.

2. **Ensemble Agent** (`ensemble.py`): A three-agent system with specialized roles:
   - **Archivist**: Extracts and organizes key information from source documents
   - **Drafter**: Creates initial synthesis based on the archivist's organization
   - **Critic**: Reviews and refines the draft for quality and completeness

## Features

- ğŸ¤– Two distinct agent architectures for document synthesis
- ğŸ“Š MLflow integration for experiment tracking and comparison
- ğŸ’° Cost and latency metrics for each approach
- ğŸ¯ LLM-as-a-judge evaluation for quality assessment
- ğŸ“ˆ Reference-free metrics (completeness, coherence, accuracy, quality)
- ğŸ“ Sample documents and synthesis tasks included

## Requirements

- Python 3.8+
- OpenAI API key
- Dependencies listed in `requirements.txt`

## Installation

1. Clone the repository:
```bash
git clone https://github.com/m-levytskyi/agent-systems-eval.git
cd agent-systems-eval
```

2. Install dependencies:
```bash
pip install -r requirements.txt
```

3. Set up your OpenAI API key:
```bash
cp .env.example .env
# Edit .env and add your OpenAI API key
```

## Usage

### Running the Evaluation

To run the complete evaluation comparing both agents:

```bash
python evaluate.py
```

This will:
- Load source documents from `data/source_documents/`
- Load synthesis tasks from `data/tasks/synthesis_tasks.json`
- Run both monolithic and ensemble agents on all tasks
- Track metrics in MLflow
- Evaluate outputs using LLM-as-a-judge
- Save all results and artifacts

### Viewing Results

After running the evaluation, view results in the MLflow UI:

```bash
mlflow ui
```

Then open http://localhost:5000 in your browser to:
- Compare runs across both agent types
- View metrics (cost, latency, quality scores)
- Examine generated outputs and intermediate results
- Analyze performance across different tasks

### Running Individual Agents

You can also run each agent independently:

**Monolithic Agent:**
```bash
python monolithic.py
```

**Ensemble Agent:**
```bash
python ensemble.py
```

## Project Structure

```
agent-systems-eval/
â”œâ”€â”€ README.md                   # This file
â”œâ”€â”€ requirements.txt            # Python dependencies
â”œâ”€â”€ .env.example               # Environment variable template
â”œâ”€â”€ .gitignore                 # Git ignore rules
â”œâ”€â”€ monolithic.py              # Single LLM agent implementation
â”œâ”€â”€ ensemble.py                # Multi-agent ensemble implementation
â”œâ”€â”€ evaluate.py                # Main evaluation script with MLflow
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ source_documents/      # Sample source documents
â”‚   â”‚   â”œâ”€â”€ doc1_ai_history.txt
â”‚   â”‚   â”œâ”€â”€ doc2_ml_fundamentals.txt
â”‚   â”‚   â””â”€â”€ doc3_ai_ethics.txt
â”‚   â””â”€â”€ tasks/                 # Synthesis task definitions
â”‚       â””â”€â”€ synthesis_tasks.json
â””â”€â”€ mlruns/                    # MLflow tracking data (generated)
```

## Metrics Tracked

### Process Metrics
- **Latency**: Total time to complete synthesis
- **Token Usage**: Prompt, completion, and total tokens
- **API Calls**: Number of LLM API calls
- **Estimated Cost**: Calculated based on token usage and model pricing

### Quality Metrics (LLM-as-a-judge)
- **Completeness**: How fully the task requirements are addressed
- **Coherence**: Clarity, logic, and structure of the writing
- **Accuracy**: Correctness and integration of information
- **Quality**: Overall professional quality
- **Overall**: Aggregate quality score

### Ensemble-Specific Metrics
- Token usage per agent (archivist, drafter, critic)
- Intermediate outputs at each stage

## Configuration

Environment variables (set in `.env`):
- `OPENAI_API_KEY`: Your OpenAI API key (required)
- `OPENAI_MODEL`: Model to use (default: gpt-4)
- `OPENAI_JUDGE_MODEL`: Model for LLM-as-a-judge (default: same as OPENAI_MODEL)

## Adding Custom Tasks

To add your own synthesis tasks:

1. Add source documents to `data/source_documents/`
2. Edit `data/tasks/synthesis_tasks.json` to add new tasks:

```json
{
  "task_id": "task4",
  "task_description": "Your synthesis task description",
  "expected_elements": [
    "Element 1",
    "Element 2"
  ]
}
```

## Expected Results

The ensemble approach typically shows:
- âœ… Higher quality scores (better organization and refinement)
- âš ï¸ Higher latency (3 sequential LLM calls)
- âš ï¸ Higher cost (more total tokens)
- âœ… Better handling of complex synthesis tasks

The monolithic approach typically shows:
- âœ… Lower latency (single LLM call)
- âœ… Lower cost (fewer tokens)
- âš ï¸ May miss nuances that benefit from specialized processing
- âœ… Efficient for straightforward tasks

## License

MIT License - see LICENSE file for details

## Contributing

Contributions are welcome! Please feel free to submit a Pull Request.